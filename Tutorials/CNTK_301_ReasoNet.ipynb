{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNTK 208: ReasoNet for Machine Comprehension\n",
    "\n",
    "## Introduction and Background\n",
    "\n",
    "This hands-on tutorial will take you through how to implement [ReasoNet](https://posenhuang.github.io/papers/reasonet_iclr_2017.pdf) in the Microsoft Cognitive Toolkit. Machine comprehension task try to find out the answer for a question given a paragraph of text. \n",
    "In this tutorial, we will use [CNN data](https://github.com/deepmind/rc-data) as an example. The data is consist of tuples (q,d,a,A). Here q is the query, d is the document, a is candidate list and A is the true answer. \n",
    "\n",
    "### Model Structure\n",
    "\n",
    "![](ReasoNet/components.png) \n",
    "![](ReasoNet/reasonet.png) \n",
    "\n",
    "## Data preparing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download data\n",
    "The data can be downloaded via (https://drive.google.com/uc?export=download&id=0BwmD_VLjROrfTTljRDVZMFJnVWM) or (https://github.com/deepmind/rc-data)\n",
    "The downloaded data is packaged as a gz file and to feed to CNTK it needs to be reformated. After unpacking the file, we will get three folders (e.g. training, test, validation), each contains a lot of files where each file is consist of a paragraph of text, a question, the answer to the questions and a list of entities. Here is an example of an instance,\n",
    "\n",
    "> __*1:*__  http://web.archive.org/web/20150731215720id_/http://edition.cnn.com/2015/04/07/sport/wladimir-klitschko-ukraine-crisis-boxing/<br/>\n",
    ">__*2:*__  <br/>\n",
    ">__*3:*__  @entity3 ( @entity2 ) @entity1 heavyweight boxing champion @entity0 has an important title defense coming up , but his thoughts continue to be dominated by the ongoing fight for democracy in @entity8 . speaking to @entity2 from his @entity3 training base ahead of the april 25 showdown with @entity12 challenger @entity11 in @entity13 , @entity0 said the crisis in his homeland has left him shocked and upset . \" my country is unfortunately suffering in the war with @entity18 -- not that @entity8 tried to give any aggression to any other nation , in this particular case @entity18 , unfortunately it 's the other way around , \" @entity0 told @entity2 . \" i never thought that our brother folk is going to have war with us , so that @entity8 and @entity18 are going to be divided with blood , \" he added . \" unfortunately , we do n't know how far it 's going to go and how worse it 's going to get . the aggression , in the military presence of ( @entity18 ) soldiers and military equipment in my country , @entity8 , is upsetting . \" @entity0 is the reigning @entity33 , @entity34 , @entity35 and @entity36 champion and has , alongside older brother @entity37 , dominated the heavyweight division in the 21st century . @entity37 , who retired from boxing in 2013 , is a prominent figure in @entity8 politics . the 43 - year - old has led the @entity43 since 2010 and was elected mayor of @entity45 in may last year . tensions in the former @entity48 state remain high despite a ceasefire agreed in february as @entity50 , led by @entity52 chancellor @entity51 and president of france @entity53 , tries to broker a peace deal between the two sides . the crisis in @entity8 began in november 2013 when former president @entity58 scuttled a trade deal with the @entity60 in favor of forging closer economic ties with @entity18 . the move triggered a wave of anti-government protests which came to a head @entity45 's @entity67 in february 2014 when clashes between protesters and government security forces left around 100 dead . the following month , @entity18 troops entered @entity8 's @entity74 peninsula before @entity18 president @entity75 completed the annexation of @entity74 -- a move denounced by most of the world as illegitimate -- after citizens of the region had voted in favor of leaving @entity8 in a referendum . more than 5,000 people have been killed in the conflict to date . \" people are dying in @entity8 every single day , \" @entity0 said . \" i do not want to see it , nobody wants to see it ... it 's hard to believe these days something like that in @entity50 -- and @entity8 is @entity50 -- can happen . \" but with the backing of the international community , @entity0 is confident @entity8 can forge a democratic future rather than slide back towards a @entity48 - era style dictatorship . \" i really wish and want this conflict to be solved and it can only be solved with @entity98 help , \" he said . \" @entity8 is looking forward to becoming a democratic country and live under @entity98 democracy . this is our decision and this is our will to get what we want . \" if somebody wants to try to put ( us ) back to the @entity48 times and be part of the @entity108 , we disagree with that . we want to be in freedom . \" we have achieved many things in moving forward and showed to the world that we do not want to live under a dictatorship . \" @entity0 , whose comments were made as part of a wide - ranging interview for @entity2 's @entity118 series , is routinely kept abreast of developments in @entity8 by brother @entity37 but also returns home whenever he can . \" as much time as i can spend , i am there in the @entity8 . it 's not like i am getting the news from mass media and making my own adjustments and judgments on what 's going on . it 's an actual presence and understanding from the inside ... it obviously affects my life , it affects the life of my family . \" the 39 - year - old and his fianc√©e @entity137 celebrated happier times last december when the @entity12 actress gave birth to a baby daughter , @entity142 . \" i need to get used to it that i 'm a father , which is really exciting . i hope i 'm going to have a big family with multiple kids , \" he said . @entity0 is n't sure when he 'll finally hang up his gloves . \" i do n't know how long i can last ... motivation and health have to be there to continue . \" but after leaving almost all his boxing opponents battered and bruised -- the @entity8 is seeking an impressive 18th consecutive title defense against @entity11 -- @entity0 is keen to carry on fighting his own and his country 's corner in the opposite way outside the ring . \" i just really want that we 'll have less violence in the world ... i hope in peace we can do anything , but if we have war then it 's definitely going to leave us dull and numb . \" watch @entity0 's @entity118 interview on @entity2 's @entity165 on wednesday april 8 at 1130 , 1245 , 1445 , 2130 , 2245 and 2345 and thursday april 9 at 0445 ( all times gmt ) and here online .<br/>\n",
    ">__*4:*__  <br/>\n",
    ">__*5:*__  @placeholder faces @entity12 challenger @entity11 in @entity13 on april 25<br/>\n",
    ">__*6:*__  <br/>\n",
    ">__*7:*__  @entity0<br/>\n",
    ">__*8:*__  <br/>\n",
    ">__*9:*__  @entity118:Human to Hero<br/>\n",
    ">__*10:*__  @entity13:New York<br/>\n",
    ">__*11:*__  @entity137:Hayden Panettiere<br/>\n",
    ">__*12:*__  @entity12:American<br/>\n",
    ">__*13:*__  @entity3:Miami<br/>\n",
    ">__*14:*__  @entity2:CNN<br/>\n",
    ">__*15:*__  @entity1:World<br/>\n",
    ">__*16:*__  @entity0:Klitschko<br/>\n",
    ">__*17:*__  @entity11:Bryant Jennings<br/>\n",
    ">__*18:*__  @entity8:Ukraine<br/>\n",
    ">__*19:*__  @entity53:Francois Hollande<br/>\n",
    ">__*20:*__  @entity52:German<br/>\n",
    ">__*21:*__  @entity51:Angela Merkel<br/>\n",
    ">__*22:*__  @entity50:Europe<br/>\n",
    ">__*23:*__  @entity75:Vladimir Putin<br/>\n",
    ">__*24:*__  @entity74:Crimea<br/>\n",
    ">__*25:*__  @entity58:Victor Yanukovych<br/>\n",
    ">__*26:*__  @entity33:IBF<br/>\n",
    ">__*27:*__  @entity35:WBO<br/>\n",
    ">__*28:*__  @entity34:WBA<br/>\n",
    ">__*29:*__  @entity37:Vitali<br/>\n",
    ">__*30:*__  @entity36:IBO<br/>\n",
    ">__*31:*__  @entity18:Russian<br/>\n",
    ">__*32:*__  @entity98:Western<br/>\n",
    ">__*33:*__  @entity108:former Soviet Union<br/>\n",
    ">__*34:*__  @entity142:Kaya<br/>\n",
    ">__*35:*__  @entity165:World Sport program<br/>\n",
    ">__*36:*__  @entity45:Kiev<br/>\n",
    ">__*37:*__  @entity43:Ukrainian Democratic Alliance for Reform<br/>\n",
    ">__*38:*__  @entity67:Maidan Square<br/>\n",
    ">__*39:*__  @entity48:Soviet<br/>\n",
    ">__*40:*__  @entity60:European Union<br/>\n",
    "\n",
    "Here line\n",
    "* __*3*__ is the paragraph\n",
    "* __*5*__ is the question\n",
    "* __*7*__ is the answer\n",
    "* __*9*__ and the rest is the entity mappings in the paragraph and query\n",
    "\n",
    "We will use the following block of code to download and merge each folder of files into a single file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All necessary data are downloaded to ../Examples/LanguageUnderstanding/ReasoNet/Data\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import os\n",
    "import re\n",
    "import requests\n",
    "import sys\n",
    "import tarfile\n",
    "import shutil\n",
    "\n",
    "def merge_files(folder, target):\n",
    "  if os.path.exists(target):\n",
    "    return\n",
    "  count = 0\n",
    "  all_files = os.listdir(folder)\n",
    "  print(\"Start to merge {0} files under folder {1} as {2}\".format(len(all_files), folder, target))\n",
    "  for f in all_files:\n",
    "    txt=os.path.join(folder, f)\n",
    "    if os.path.isfile(txt):\n",
    "      with open(txt) as sample:\n",
    "        content = sample.readlines()\n",
    "        context = content[2].strip()\n",
    "        query = content[4].strip()\n",
    "        answer = content[6].strip()\n",
    "        entities = []\n",
    "        for k in range(8, len(content)):\n",
    "          entities += [ content[k].strip() ]\n",
    "        with open(target, 'a') as output:\n",
    "          output.write(\"{0}\\t{1}\\t{2}\\t{3}\\n\".format(query, answer, context, \"\\t\".join(entities)))\n",
    "    count+=1\n",
    "    if count%1000==0:\n",
    "      sys.stdout.write(\".\")\n",
    "      sys.stdout.flush()\n",
    "  print()\n",
    "  print(\"Finished to merge {0}\".format(target))\n",
    "\n",
    "def download_cnn(target=\".\"):\n",
    "  if os.path.exists(os.path.join(target, \"cnn\")):\n",
    "    shutil.rmtree(os.path.join(target, \"cnn\"))\n",
    "  if not os.path.exists(target):\n",
    "    os.makedirs(target)\n",
    "  url=\"https://drive.google.com/uc?export=download&id=0BwmD_VLjROrfTTljRDVZMFJnVWM\"\n",
    "  print(\"Start to download CNN data from {0} to {1}\".format(url, target))\n",
    "  pre_request = requests.get(url)\n",
    "  confirm_match = re.search(r\"confirm=(.{4})\", pre_request.content.decode(\"utf-8\"))\n",
    "  confirm_url = url + \"&confirm=\" + confirm_match.group(1)\n",
    "  download_request = requests.get(confirm_url, cookies=pre_request.cookies)\n",
    "  tar = tarfile.open(mode=\"r:gz\", fileobj=io.BytesIO(download_request.content))\n",
    "  tar.extractall(target)\n",
    "  print(\"Finished to download {0} to {1}\".format(url, target))\n",
    "\n",
    "def file_exists(src):\n",
    "  return (os.path.isfile(src) and os.path.exists(src))\n",
    "\n",
    "data_path = \"../Examples/LanguageUnderstanding/ReasoNet/Data\"\n",
    "raw_train_data=os.path.join(data_path, \"cnn/training.txt\")\n",
    "raw_test_data=os.path.join(data_path, \"cnn/test.txt\")\n",
    "raw_validation_data=os.path.join(data_path, \"cnn/validation.txt\")\n",
    "if not (file_exists(raw_train_data) and file_exists(raw_test_data) and file_exists(raw_validation_data)):\n",
    "  download_cnn(data_path)\n",
    "\n",
    "merge_files(os.path.join(data_path, \"cnn/questions/training\"), raw_train_data)\n",
    "merge_files(os.path.join(data_path, \"cnn/questions/test\"), raw_test_data)\n",
    "merge_files(os.path.join(data_path, \"cnn/questions/validation\"), raw_validation_data)\n",
    "print(\"All necessary data are downloaded to {0}\".format(data_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to CNTK Text Format\n",
    "\n",
    "In order to take advantage of the scalable readers bundled with CNTK, we need to convert the original data into a column separated format [CNTK text format](https://github.com/Microsoft/CNTK/wiki/BrainScript-CNTKTextFormat-Reader). \n",
    "There are 5 columns/streams in the conveted CTF data file, including context, query, entity indication, label, entity ids. Here is a snippnet of the converted CTF output for the above example input,\n",
    "\n",
    ">0 |Q 12:1 |C 4:1 |E 1 |L 0 |EID 4:1<br/>\n",
    "> |Q 1739:1 |C 626:1 |E 0 |L 0 |EID 2:1<br/>\n",
    "> |Q 14:1 |C 2:1 |E 1 |L 0 |EID 1:1<br/>\n",
    "> |Q 5453:1 |C 625:1 |E 0 |L 0 |EID 3:1<br/>\n",
    "> |Q 13:1 |C 1:1 |E 1 |L 0 |EID 9:1<br/>\n",
    "> |Q 594:1 |C 7562:1 |E 0 |L 0 |EID 2:1<br/>\n",
    "> |Q 15:1 |C 5284:1 |E 0 |L 0 |EID 4:1<br/>\n",
    "> |Q 600:1 |C 1245:1 |E 0 |L 0 |EID 14:1<br/>\n",
    "> |Q 1307:1 |C 3:1 |E 1 |L 1 |EID 13:1<br/>\n",
    "> |Q 1309:1 |C 616:1 |E 0 |L 0 |EID 15:1<br/>\n",
    "> |C 620:1 |E 0 |L 0 |EID 3:1<br/>\n",
    "> |C 927:1 |E 0 |L 0 |EID 20:1<br/>\n",
    "> |C 1115:1 |E 0 |L 0 |EID 9:1<br/>\n",
    "> |C 1017:1 |E 0 |L 0 |EID 20:1<br/>\n",
    "> |C 1067:1 |E 0 |L 0 |EID 3:1<br/>\n",
    "> |C 650:1 |E 0 |L 0 |EID 2:1<br/>\n",
    "> |C 587:1 |E 0 |L 0 |EID 9:1<br/>\n",
    "> |C 613:1 |E 0 |L 0 |EID 20:1<br/>\n",
    "> |C 608:1 |E 0 |L 0 |EID 20:1<br/>\n",
    "> |C 2892:1 |E 0 |L 0 |EID 9:1<br/>\n",
    "> |C 1015:1 |E 0 |L 0 |EID 3:1<br/>\n",
    "> |C 589:1 |E 0 |L 0 |EID 35:1<br/>\n",
    "> |C 615:1 |E 0 |L 0 |EID 36:1<br/>\n",
    "> |C 2814:1 |E 0 |L 0 |EID 37:1<br/>\n",
    "> |C 617:1 |E 0 |L 0 |EID 39:1<br/>\n",
    "> |C 586:1 |E 0 |L 0 |EID 40:1<br/>\n",
    "> |C 2090:1 |E 0 |L 0 |EID 40:1<br/>\n",
    "> |C 1057:1 |E 0 |L 0 |EID 9:1<br/>\n",
    "> |C 597:1 |E 0 |L 0 |EID 44:1<br/>\n",
    "> |C 2054:1 |E 0 |L 0 |EID 47:1<br/>\n",
    "\n",
    "The first column is the sequence id, 0. The second is the features of Query, the third is the features of Context, the fourth is a boolean to indicate if that word in the Context is an entity, the fifth is the Label which indicate if that word in the context is the answer. The last is the ID of entities in the context.\n",
    "\n",
    "The code below performs the conversion. \n",
    "\n",
    "**Note**: The downloading and conversion can take upto 30 min and requires 11 GB of local disc space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data conversion finished.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import math\n",
    "import functools\n",
    "import numpy as np\n",
    "\n",
    "class WordFreq:\n",
    "  def __init__(self, word, id, freq):\n",
    "    self.word = word\n",
    "    self.id = id\n",
    "    self.freq = freq\n",
    "\n",
    "class Vocabulary:\n",
    "  \"\"\"Build word vocabulary with frequency\"\"\"\n",
    "  def __init__(self, name):\n",
    "    self.name = name\n",
    "    self.size = 0\n",
    "    self.__dict = {}\n",
    "    self.__has_index = False\n",
    "\n",
    "  def push(self, word):\n",
    "    if word in self.__dict:\n",
    "      self.__dict[word].freq += 1\n",
    "    else:\n",
    "      self.__dict[word] = WordFreq(word, len(self.__dict), 1)\n",
    "\n",
    "  def build_index(self, max_size):\n",
    "    def word_cmp(x, y):\n",
    "      if x.freq == y.freq :\n",
    "        return (x.word > y.word) - (x.word < y.word)\n",
    "      else:\n",
    "        return x.freq - y.freq\n",
    "\n",
    "    items = sorted(self.__dict.values(), key=functools.cmp_to_key(word_cmp), reverse=True)\n",
    "    if len(items)>max_size:\n",
    "      del items[max_size:]\n",
    "    self.size=len(items)\n",
    "    self.__dict.clear()\n",
    "    for it in items:\n",
    "      it.id = len(self.__dict)\n",
    "      self.__dict[it.word] = it\n",
    "    self.__has_index = True\n",
    "\n",
    "  def save(self, dst):\n",
    "    if not self.__has_index:\n",
    "      self.build_index(sys.maxsize)\n",
    "    if self.name != None:\n",
    "      dst.write(\"{0}\\t{1}\\n\".format(self.name, self.size))\n",
    "    for it in sorted(self.__dict.values(), key=lambda it:it.id):\n",
    "      dst.write(\"{0}\\t{1}\\t{2}\\n\".format(it.word, it.id, it.freq))\n",
    "\n",
    "  def load(self, src):\n",
    "    line = src.readline()\n",
    "    if line == \"\":\n",
    "      return\n",
    "    line = line.rstrip('\\n')\n",
    "    head = line.split()\n",
    "    max_size = sys.maxsize\n",
    "    if len(head) == 2:\n",
    "      self.name = head[0]\n",
    "      max_size = int(head[1])\n",
    "    cnt = 0\n",
    "    while cnt < max_size:\n",
    "      line = src.readline()\n",
    "      if line == \"\":\n",
    "        break\n",
    "      line = line.rstrip('\\n')\n",
    "      items = line.split()\n",
    "      self.__dict[items[0]] = WordFreq(items[0], int(items[1]), int(items[2]))\n",
    "      cnt += 1\n",
    "    self.size = len(self.__dict)\n",
    "    self.__has_index = True\n",
    "\n",
    "  def __getitem__(self, key):\n",
    "    if key in self.__dict:\n",
    "      return self.__dict[key]\n",
    "    else:\n",
    "      return None\n",
    "\n",
    "  def values(self):\n",
    "    return self.__dict.values()\n",
    "\n",
    "  def __len__(self):\n",
    "    return self.size\n",
    "\n",
    "  def __contains__(self, q):\n",
    "    return q in self.__dict\n",
    "\n",
    "  @staticmethod\n",
    "  def is_cnn_entity(word):\n",
    "    return word.startswith('@entity') or word.startswith('@placeholder')\n",
    "\n",
    "  @staticmethod\n",
    "  def load_vocab(vocab_src):\n",
    "    \"\"\"\n",
    "    Loa vocabulary from file.\n",
    "\n",
    "    Args:\n",
    "      vocab_src (`str`): the file stored with the vocabulary data\n",
    "      \n",
    "    Returns:\n",
    "      :class:`Vocabulary`: Vocabulary of the entities\n",
    "      :class:`Vocabulary`: Vocabulary of the words\n",
    "    \"\"\"\n",
    "    word_vocab = Vocabulary(\"WordVocab\")\n",
    "    entity_vocab = Vocabulary(\"EntityVocab\")\n",
    "    with open(vocab_src, 'r', encoding='utf-8') as src:\n",
    "      entity_vocab.load(src)\n",
    "      word_vocab.load(src)\n",
    "    return entity_vocab, word_vocab\n",
    "\n",
    "  @staticmethod\n",
    "  def build_vocab(input_src, vocab_dst, max_size=50000):\n",
    "    \"\"\"\n",
    "    Build vocabulary from raw corpus file.\n",
    "\n",
    "    Args:\n",
    "      input_src (`str`): the path of the corpus file\n",
    "      vocab_dst (`str`): the path of the vocabulary file to save the built vocabulary\n",
    "      max_size (`int`): the maxium size of the word vocabulary\n",
    "    Returns:\n",
    "      :class:`Vocabulary`: Vocabulary of the entities\n",
    "      :class:`Vocabulary`: Vocabulary of the words\n",
    "    \"\"\"\n",
    "    # Leave the first as Unknown\n",
    "    max_size -= 1\n",
    "    word_vocab = Vocabulary(\"WordVocab\")\n",
    "    entity_vocab = Vocabulary(\"EntityVocab\")\n",
    "    linenum = 0\n",
    "    print(\"Start build vocabulary from {0} with maxium words {1}. Saved to {2}\"\\\n",
    "          .format(input_src, max_size, vocab_dst))\n",
    "    with open(input_src, 'r', encoding='utf-8') as src:\n",
    "      all_lines = src.readlines()\n",
    "      print(\"Total lines to process: {0}\".format(len(all_lines)))\n",
    "      for line in all_lines:\n",
    "        line = line.strip('\\n')\n",
    "        ans, query_words, context_words = Vocabulary.parse_corpus_line(line)\n",
    "        for q in query_words:\n",
    "          if Vocabulary.is_cnn_entity(q):\n",
    "          #if q.startswith('@'):\n",
    "            entity_vocab.push(q)\n",
    "          else:\n",
    "            word_vocab.push(q)\n",
    "        for q in context_words:\n",
    "          #if q.startswith('@'):\n",
    "          if Vocabulary.is_cnn_entity(q):\n",
    "            entity_vocab.push(q)\n",
    "          else:\n",
    "            word_vocab.push(q)\n",
    "        linenum += 1\n",
    "        if linenum%1000==0:\n",
    "          sys.stdout.write(\".\")\n",
    "          sys.stdout.flush()\n",
    "    print()\n",
    "    entity_vocab.build_index(max_size)\n",
    "    word_vocab.build_index(max_size)\n",
    "    with open(vocab_dst, 'w', encoding='utf-8') as dst:\n",
    "      entity_vocab.save(dst)\n",
    "      word_vocab.save(dst)\n",
    "    print(\"Finished to generate vocabulary from: {0}\".format(input_src))\n",
    "    return entity_vocab, word_vocab\n",
    "\n",
    "  @staticmethod\n",
    "  def parse_corpus_line(line):\n",
    "    \"\"\"\n",
    "    Parse bing corpus line to answer, query and context.\n",
    "\n",
    "    Args:\n",
    "      line (`str`): A line of text of bing corpus\n",
    "    Returns:\n",
    "      :`str`: Answer word\n",
    "      :`str[]`: Array of query words\n",
    "      :`str[]`: Array of context/passage words\n",
    "\n",
    "    \"\"\"\n",
    "    data = line.split('\\t')\n",
    "    query = data[0]\n",
    "    answer = data[1]\n",
    "    context = data[2]\n",
    "    query_words = query.split()\n",
    "    context_words = context.split()\n",
    "    return answer, query_words, context_words\n",
    "\n",
    "  def build_corpus(entities, words, corpus, output, max_seq_len=100000):\n",
    "    \"\"\"\n",
    "    Build featurized corpus and store it in CNTK Text Format.\n",
    "\n",
    "    Args:\n",
    "      entities (class:`Vocabulary`): The entities vocabulary\n",
    "      words (class:`Vocabulary`): The words vocabulary\n",
    "      corpus (`str`): The file path of the raw corpus\n",
    "      output (`str`): The file path to store the featurized corpus data file\n",
    "    \"\"\"\n",
    "    seq_id = 0\n",
    "    print(\"Start to build CTF data from: {0}\".format(corpus))\n",
    "    with open(corpus, 'r', encoding = 'utf-8') as corp:\n",
    "      with open(output, 'w', encoding = 'utf-8') as outf:\n",
    "        all_lines = corp.readlines()\n",
    "        print(\"Total lines to prcess: {0}\".format(len(all_lines)))\n",
    "        for line in all_lines:\n",
    "          line = line.strip('\\n')\n",
    "          ans, query_words, context_words = Vocabulary.parse_corpus_line(line)\n",
    "          ans_item = entities[ans]\n",
    "          query_ids = []\n",
    "          context_ids = []\n",
    "          is_entity = []\n",
    "          entity_ids = []\n",
    "          labels = []\n",
    "          pos = 0\n",
    "          answer_idx = None\n",
    "          for q in context_words:\n",
    "            if Vocabulary.is_cnn_entity(q):\n",
    "              item = entities[q]\n",
    "              context_ids += [ item.id + 1 ]\n",
    "              entity_ids += [ item.id + 1 ]\n",
    "              is_entity += [1]\n",
    "              if ans_item.id == item.id:\n",
    "                labels += [1]\n",
    "                answer_idx = pos\n",
    "              else:\n",
    "                labels += [0]\n",
    "            else:\n",
    "              item = words[q]\n",
    "              context_ids += [ (item.id + 1 + entities.size) if item != None else 0 ]\n",
    "              is_entity += [0]\n",
    "              labels += [0]\n",
    "            pos += 1\n",
    "            if (pos >= max_seq_len):\n",
    "              break\n",
    "          if answer_idx is None:\n",
    "            continue\n",
    "          for q in query_words:\n",
    "            if Vocabulary.is_cnn_entity(q):\n",
    "              item = entities[q]\n",
    "              query_ids += [ item.id + 1 ]\n",
    "            else:\n",
    "              item = words[q]\n",
    "              query_ids += [ (item.id + 1 + entities.size) if item != None else 0 ]\n",
    "          #Write featurized ids\n",
    "          outf.write(\"{0}\".format(seq_id))\n",
    "          for i in range(max(len(context_ids), len(query_ids))):\n",
    "            if i < len(query_ids):\n",
    "              outf.write(\" |Q {0}:1\".format(query_ids[i]))\n",
    "            if i < len(context_ids):\n",
    "              outf.write(\" |C {0}:1\".format(context_ids[i]))\n",
    "              outf.write(\" |E {0}\".format(is_entity[i]))\n",
    "              outf.write(\" |L {0}\".format(labels[i]))\n",
    "            if i < len(entity_ids):\n",
    "              outf.write(\" |EID {0}:1\".format(entity_ids[i]))\n",
    "            outf.write(\"\\n\")\n",
    "          seq_id += 1\n",
    "          if seq_id%1000 == 0:\n",
    "            sys.stdout.write(\".\")\n",
    "            sys.stdout.flush()\n",
    "    print()\n",
    "    print(\"Finished to build corpus from {0}\".format(corpus))\n",
    "  \n",
    "vocab_path=os.path.join(data_path, \"cnn/cnn.vocab\")\n",
    "train_ctf=os.path.join(data_path, \"cnn/training.ctf\")\n",
    "test_ctf=os.path.join(data_path, \"cnn/test.ctf\")\n",
    "validation_ctf=os.path.join(data_path, \"cnn/validation.ctf\")\n",
    "vocab_size=101000\n",
    "if not (file_exists(train_ctf) and file_exists(test_ctf) and file_exists(validation_ctf)):\n",
    "  entity_vocab, word_vocab = Vocabulary.build_vocab(raw_train_data, vocab_path, vocab_size)\n",
    "  Vocabulary.build_corpus(entity_vocab, word_vocab, raw_train_data, train_ctf)\n",
    "  Vocabulary.build_corpus(entity_vocab, word_vocab, raw_test_data, test_ctf)\n",
    "  Vocabulary.build_corpus(entity_vocab, word_vocab, raw_validation_data, validation_ctf)\n",
    "print(\"Training data conversion finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic CNTK imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import cntk\n",
    "from cntk import Trainer, Axis, device, combine\n",
    "from cntk.layers.blocks import Stabilizer, _initializer_for,  _INFERRED, Parameter, Placeholder, GRU,Input\n",
    "from cntk.layers import Recurrence, Convolution\n",
    "from cntk.ops import input_variable, cross_entropy_with_softmax, classification_error, sequence, reduce_sum, \\\n",
    "    parameter, times, element_times, past_value, plus, placeholder_variable, reshape, constant, sigmoid, \\\n",
    "    convolution, tanh, times_transpose, greater, cosine_distance, element_divide, element_select, exp, \\\n",
    "    future_value, past_value\n",
    "from cntk.internal import _as_tuple, sanitize_input\n",
    "from cntk.initializer import uniform, glorot_uniform\n",
    "from cntk.io import MinibatchSource, CTFDeserializer, StreamDef, StreamDefs, INFINITELY_REPEAT, \\\n",
    "    DEFAULT_RANDOMIZATION_WINDOW\n",
    "import cntk.ops as ops\n",
    "import cntk.learners as learners\n",
    "# Check for an environment variable defined in CNTK's test infrastructure\n",
    "envvar = 'CNTK_EXTERNAL_TESTDATA_SOURCE_DIRECTORY'\n",
    "def is_test(): return envvar in os.environ\n",
    "\n",
    "# Select the right target device when this notebook is being tested\n",
    "# Currently supported only for GPU \n",
    "\n",
    "if 'TEST_DEVICE' in os.environ:\n",
    "    if os.environ['TEST_DEVICE'] == 'cpu':\n",
    "        raise ValueError('This notebook is currently not support on CPU') \n",
    "    else:\n",
    "        cntk.device.set_default_device(cntk.device.gpu(0))\n",
    "cntk.device.set_default_device(cntk.device.gpu(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils\n",
    "Some utils will used during model creation and training stage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logger\n",
    "We use logger to write information both to console and a disk file, so that we can check the inforamtion after the process exited."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import math\n",
    "\n",
    "class logger:\n",
    "  __name=''\n",
    "  __logfile=''\n",
    "\n",
    "  @staticmethod\n",
    "  def init(name=''):\n",
    "    if not os.path.exists(\"model\"):\n",
    "      os.mkdir(\"model\")\n",
    "    if not os.path.exists(\"log\"):\n",
    "      os.mkdir(\"log\")\n",
    "    if name=='' or name is None:\n",
    "      logger.__name='train'\n",
    "    logger.__logfile = 'log/{}_{}.log'.format(logger.__name, datetime.now().strftime(\"%m-%d_%H.%M.%S\"))\n",
    "    if os.path.exists(logger.__logfile):\n",
    "      os.remove(logger.__logfile)\n",
    "    print('Log with log file: {0}'.format(logger.__logfile))\n",
    "\n",
    "  @staticmethod\n",
    "  def log(message, toconsole=True):\n",
    "    if logger.__logfile == '' or logger.__logfile is None:\n",
    "      logger.init()\n",
    "    if toconsole:\n",
    "      print(message)\n",
    "    with open(logger.__logfile, 'a') as logf:\n",
    "      logf.write(\"{}| {}\\n\".format(datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), message))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding\n",
    "In this implementation we apply a special policy to train the embedding layer. For entities in the context/paragraph, we just use fixed random vectors as the embedding of them and won't update them during training stage. For other words in the context/paragraph and query, we will initialize the embedding using glorot uniform initialization or loading from an existing embedding matrix, e.g. glove embedding and they will be updated during training stage. To approch this, we can't simply adopt existing initializer or embedding lookup funciton in CNTK, and we implimented  `create_random_matrix` and `load_embedding` to create *random initialization matrix* and *load existing embedding matrix*. The class `uniform_initializer` will be used by `load_embedding` to initialize *enities* and other words that can't be found in the existing embedding matrix (looking up table)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class uniform_initializer:\n",
    "  def __init__(self, scale=1, bias=0, seed=0):\n",
    "    self.seed = seed\n",
    "    self.scale = scale\n",
    "    self.bias = bias\n",
    "    np.random.seed(self.seed)\n",
    "\n",
    "  def reset(self):\n",
    "    np.random.seed(self.seed)\n",
    "\n",
    "  def next(self, size=None):\n",
    "    return np.random.uniform(0, 1, size)*self.scale + self.bias\n",
    "\n",
    "def create_random_matrix(rows, columns):\n",
    "  scale = math.sqrt(6/(rows+columns))*2\n",
    "  rand = uniform_initializer(scale, -scale/2)\n",
    "  embedding = [None]*rows\n",
    "  for i in range(rows):\n",
    "    embedding[i] = np.array(rand.next(columns), dtype=np.float32)\n",
    "  return np.ndarray((rows, columns), dtype=np.float32, buffer=np.array(embedding))\n",
    "\n",
    "def load_embedding(embedding_path, vocab_path, dim, init=None):\n",
    "  entity_vocab, word_vocab = Vocabulary.load_bingvocab(vocab_path)\n",
    "  vocab_dim = len(entity_vocab) + len(word_vocab) + 1\n",
    "  entity_size = len(entity_vocab)\n",
    "  item_embedding = [None]*vocab_dim\n",
    "  with open(embedding_path, 'r') as embedding:\n",
    "    for line in embedding.readlines():\n",
    "      line = line.strip('\\n')\n",
    "      item = line.split(' ')\n",
    "      if item[0] in word_vocab:\n",
    "        item_embedding[word_vocab[item[0]].id + entity_size + 1] = \\\n",
    "        np.array(item[1:], dtype=\"|S\").astype(np.float32)\n",
    "  if init != None:\n",
    "    init.reset()\n",
    "\n",
    "  for i in range(vocab_dim):\n",
    "    if item_embedding[i] is None:\n",
    "      if init:\n",
    "        item_embedding[i] = np.array(init.next(dim), dtype=np.float32)\n",
    "      else:\n",
    "        item_embedding[i] = np.array([0]*dim, dtype=np.float32)\n",
    "  return np.ndarray((vocab_dim, dim), dtype=np.float32, buffer=np.array(item_embedding))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic components\n",
    "Here we provide some basic components that will be used in the model to simplify the model creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Cosine Similarity\n",
    "In ReasoNet we use project cosine similarity to compute the attention between *internal status* and each words in the *external memory* where *external memory* is composited of *paragraph memory* and *query memory*. The formula for *doc attention* can be wrote as,\n",
    "$$\n",
    "a_{t,i}^{doc}=softmax_{i=1,...,\\left|M^{doc}\\right|}{\\gamma cos\\left(w_1^{doc}m_i^{doc}, w_2^{doc}s_t\\right)}\n",
    "$$\n",
    "And the formular for *query attention* is similary as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cosine_similarity(src, tgt, name=''):\n",
    "  \"\"\"\n",
    "  Compute the cosine similarity of two squences.\n",
    "  Src is a sequence of length 1\n",
    "  Tag is a sequence of lenght >=1\n",
    "  \"\"\"\n",
    "  src_br = sequence.broadcast_as(src, tgt, name='src_broadcast')\n",
    "  sim = cosine_distance(src_br, tgt, name)\n",
    "  return sim\n",
    "\n",
    "def project_cosine_sim(att_dim, init = glorot_uniform(), name=''):\n",
    "  \"\"\"\n",
    "  Compute the project cosine similarity of two input sequences, \n",
    "  where each of the input will be projected to a new dimention space (att_dim) via Wi/Wm\n",
    "  \"\"\"\n",
    "  Wi = Parameter(_INFERRED + tuple((att_dim,)), init = init, name='Wi')\n",
    "  Wm = Parameter(_INFERRED + tuple((att_dim,)), init = init, name='Wm')\n",
    "  status = placeholder_variable(name='status')\n",
    "  memory = placeholder_variable(name='memory')\n",
    "  projected_status = times(status, Wi, name = 'projected_status')\n",
    "  projected_memory = times(memory, Wm, name = 'projected_memory')\n",
    "  sim = cosine_similarity(projected_status, projected_memory, name= name+ '_sim')\n",
    "  return sequence.softmax(sim, name = name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temination gate\n",
    "The formula used to compute the termination probability in each time step is as,\n",
    "$$\n",
    "f_t\\left(s_t;\\theta_t\\right)=sigmoid\\left(w_ts_t+b_t\\right), where\\ \\theta_t=\\left(w_t, b_t\\right)\n",
    "$$\n",
    "In our implementaiton, we ignored bias $b_t$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def termination_gate(init = glorot_uniform(), name=''):\n",
    "  Wt = Parameter( _INFERRED + tuple((1,)), init = init, name='Wt')\n",
    "  status = placeholder_variable(name='status')\n",
    "  return sigmoid(times(status, Wt), name=name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Model\n",
    "\n",
    "#### Model parameters\n",
    "We use `model_params` to wrapper the parameters to create the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class model_params:\n",
    "  def __init__(self, vocab_dim, entity_dim, hidden_dim, embedding_dim=100, embedding_init=None, \n",
    "               share_rnn_param=False, max_rl_steps=5, dropout_rate=None, att_dim=384, \n",
    "               init=glorot_uniform(), model_name='rsn'):\n",
    "    self.vocab_dim = vocab_dim\n",
    "    self.entity_dim = entity_dim\n",
    "    self.hidden_dim = hidden_dim\n",
    "    self.embedding_dim = embedding_dim\n",
    "    self.embedding_init = embedding_init\n",
    "    self.max_rl_steps = max_rl_steps\n",
    "    self.dropout_rate = dropout_rate\n",
    "    self.init = init\n",
    "    self.model_name = model_name\n",
    "    self.share_rnn_param = share_rnn_param\n",
    "    self.attention_dim = att_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attention model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def attention_model(context_memory, query_memory, init_status, hidden_dim, att_dim, \n",
    "                    max_steps = 5, init = glorot_uniform()):\n",
    "  \"\"\"\n",
    "  Create the attention model for reasonet\n",
    "  Args:\n",
    "    context_memory: Context memory\n",
    "    query_memory: Query memory\n",
    "    init_status: Intialize status\n",
    "    hidden_dim: The dimention of hidden state\n",
    "    att_dim: The dimention of attention\n",
    "    max_step: Maxuim number of step to revisit the context memory\n",
    "  \"\"\"\n",
    "  gru = GRU((hidden_dim*2, ), name='control_status')\n",
    "  status = init_status\n",
    "  output = [None]*max_steps*2\n",
    "  sum_prob = None\n",
    "  context_cos_sim = project_cosine_sim(att_dim, name='context_attention')\n",
    "  query_cos_sim = project_cosine_sim(att_dim, name='query_attention')\n",
    "  ans_cos_sim = project_cosine_sim(att_dim, name='candidate_attention')\n",
    "  stop_gate = termination_gate(name='terminate_prob')\n",
    "  prev_stop = 0\n",
    "  for step in range(max_steps):\n",
    "    context_attention_weight = context_cos_sim(status, context_memory)\n",
    "    query_attention_weight = query_cos_sim(status, query_memory)\n",
    "    context_attention = sequence.reduce_sum(times(context_attention_weight, context_memory), name='C-Att')\n",
    "    query_attention = sequence.reduce_sum(times(query_attention_weight, query_memory), name='Q-Att')\n",
    "    attention = ops.splice(query_attention, context_attention, name='att-sp')\n",
    "    status = gru(status, attention).output\n",
    "    termination_prob = stop_gate(status)\n",
    "    ans_attention = ans_cos_sim(status, context_memory)\n",
    "    output[step*2] = ans_attention\n",
    "    if step < max_steps -1:\n",
    "      stop_prob = prev_stop + ops.log(termination_prob, name='log_stop')\n",
    "    else:\n",
    "      stop_prob = prev_stop\n",
    "    output[step*2+1] = sequence.broadcast_as(ops.exp(stop_prob, name='exp_log_stop'), \n",
    "                                             output[step*2], name='Stop_{0}'.format(step))\n",
    "    prev_stop += ops.log(1-termination_prob, name='log_non_stop')\n",
    "\n",
    "  final_ans = None\n",
    "  for step in range(max_steps):\n",
    "    if final_ans is None:\n",
    "      final_ans = output[step*2] * output[step*2+1]\n",
    "    else:\n",
    "      final_ans += output[step*2] * output[step*2+1]\n",
    "  combine_func = combine(output + [ final_ans ], name='Attention_func')\n",
    "  return combine_func"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define  the network\n",
    "#### Dynamic axes in CNTK (Key concept)\n",
    "One of the important concepts in understanding CNTK is the idea of two types of axes:\n",
    "* static axes, which are the traditional axes of a variable's shape, and\n",
    "* dynamic axes, which have dimensions that are unknown until the variable is bound to real data at computation time.\n",
    "\n",
    "The dynamic axes are particularly important in the world of recurrent neural networks. Instead of having to decide a maximum sequence length ahead of time, padding your sequences to that size, and wasting computation, CNTK's dynamic axes allow for variable sequence lengths that are automatically packed in minibatches to be as efficient as possible.\n",
    "\n",
    "When setting up sequences, there are two dynamic axes that are important to consider. The first is the batch axis, which is the axis along which multiple sequences are batched. The second is the dynamic axis particular to that sequence. The latter is specific to a particular input because of variable sequence lengths in your data. In CNTK, we use the dynamic axe name to idenitify different dynamic axes, and all sequence oprations between different variables require them have the same dynamic axes which means they must have the same length on all the axes. \n",
    "\n",
    "In ReasoNet networks, we have five input streams/sequences: *query*, *paragraph*, *label*, *entity id*, *entity indicator*, where *entity id* and *entity indicator* are helper sequences. *Query*, *paragraph* and *entity id* have different sequence lengths so they have different sequence dynamic axis. *label* and *entity id* have the same sequence length as *paragraph*, so they share the same dynamic axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_model(params):\n",
    "  \"\"\"\n",
    "  Create ReasoNet model\n",
    "  Args:\n",
    "    params (class:`model_params`): The parameters used to create the model\n",
    "  \"\"\"\n",
    "  logger.log(\"Create model: dropout_rate: {0}, init:{1}, embedding_init: {2}\"\\\n",
    "             .format(params.dropout_rate, params.init, params.embedding_init))\n",
    "  # Query and Doc/Context/Paragraph inputs to the model\n",
    "  batch_axis = Axis.default_batch_axis()\n",
    "  query_seq_axis = Axis('queryAxis')\n",
    "  context_seq_axis = Axis('contextAxis')\n",
    "  query_dynamic_axes = [batch_axis, query_seq_axis]\n",
    "  query_sequence = Input(shape=(params.vocab_dim), is_sparse=True, \n",
    "                                  dynamic_axes=query_dynamic_axes, name='query')\n",
    "  context_dynamic_axes = [batch_axis, context_seq_axis]\n",
    "  context_sequence = Input(shape=(params.vocab_dim), is_sparse=True, \n",
    "                                    dynamic_axes=context_dynamic_axes, name='context')\n",
    "  # entitiy ids mask is a sequence with the same length as context sequence where each iterm is and indicator of\n",
    "  # wether the corresponding word in the context is an entity or not.\n",
    "  entity_ids_mask = Input(shape=(1,), is_sparse=False, dynamic_axes=context_dynamic_axes, \n",
    "                                   name='entity_ids_mask')\n",
    "  # embedding\n",
    "  if params.embedding_init is None:\n",
    "    embedding_init = create_random_matrix(params.vocab_dim, params.embedding_dim)\n",
    "  else:\n",
    "    embedding_init = params.embedding_init\n",
    "  embedding = parameter(shape=(params.vocab_dim, params.embedding_dim), init=None)\n",
    "  embedding.value = embedding_init\n",
    "  constant_embedding = constant(embedding_init, shape=(params.vocab_dim, params.embedding_dim))\n",
    "\n",
    "  if params.dropout_rate is not None:\n",
    "    query_embedding  = ops.dropout(times(query_sequence , embedding), params.dropout_rate, \n",
    "                                   name='query_embedding')\n",
    "    context_embedding = ops.dropout(times(context_sequence, embedding), params.dropout_rate, \n",
    "                                    name='context_embedding')\n",
    "  else:\n",
    "    query_embedding  = times(query_sequence , embedding, name='query_embedding')\n",
    "    context_embedding = times(context_sequence, embedding, name='context_embedding')\n",
    "\n",
    "  contextGruW = Parameter(_INFERRED +  _as_tuple(params.hidden_dim), init=glorot_uniform(), \n",
    "                          name='gru_params')\n",
    "  queryGruW = Parameter(_INFERRED +  _as_tuple(params.hidden_dim), init=glorot_uniform(), \n",
    "                        name='gru_params')\n",
    "  # We use constant random vectors as the embedding of entities in the paragraph, \n",
    "  # as we treat them as meaningless symbolic in the paragraph which is equal to entity shuffle\n",
    "  entity_embedding = ops.times(context_sequence, constant_embedding, name='constant_entity_embedding')\n",
    "  \n",
    "  # Unlike other words in the context, \n",
    "  # we keep the entity vectors fixed as a random vector so that each vector just means an identifier \n",
    "  # of different entities in the context and it has no semantic meaning\n",
    "  full_context_embedding = ops.element_select(entity_ids_mask, entity_embedding, context_embedding)\n",
    "  context_memory = ops.optimized_rnnstack(full_context_embedding, contextGruW, params.hidden_dim, 1, \n",
    "                                          True, recurrent_op='gru', name='context_mem')\n",
    "\n",
    "  query_memory = ops.optimized_rnnstack(query_embedding, queryGruW, params.hidden_dim, 1, True, \n",
    "                                        recurrent_op='gru', name='query_mem')\n",
    "  qfwd = ops.slice(sequence.last(query_memory), -1, 0, params.hidden_dim, name='fwd')\n",
    "  qbwd = ops.slice(sequence.first(query_memory), -1, params.hidden_dim, params.hidden_dim*2, name='bwd')\n",
    "  init_status = ops.splice(qfwd, qbwd, name='Init_Status') # get last fwd status and first bwd status\n",
    "  return attention_model(context_memory, query_memory, init_status, params.hidden_dim, \n",
    "                         params.attention_dim, max_steps = params.max_rl_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss fucntion\n",
    "#### Contractive Reward\n",
    "\n",
    "In the ReasoNet paper, it gives the fomula of the Reward as\n",
    "\\begin{align}\n",
    "J(\\theta) = \\mathbf{E}_{\\pi\\left(t_{1:T},a_T;\\theta\\right)}\\left[\\sum_{t=1}^Tr_t\\right]\n",
    "\\end{align}\n",
    "\n",
    "And it applies REINFORCE algorithm to estimate \n",
    "\\begin{align} \n",
    "\\nabla_{\\theta}J(\\theta) = \\mathbf{E}_{\\pi\\left(t_{1:T},a_T;\\theta\\right)}\\left[\\nabla_{\\theta}log_{\\pi}\\left(t_{1:T},a_T;\\theta\\right)r_T\\right]=\\sum_{\\left(t_{1:T},a_T\\right)\\in\\mathbb{A}^+}\\pi\\left(t_{1:T},a_T;\\theta\\right)\\left[\\nabla_{\\theta}log\\pi\\left(t_{1:T},a_T;\\theta\\right)\\left(r_T-b_T\\right)\\right]\n",
    "\\end{align}\n",
    "\n",
    "However, as the baseline $\\left\\{b_T;T=1...T_{max}\\right\\}$ are global variables independent of instances, it leads to slow convergence in training ReasoNet. Instead, the paper rewrite the formular as,\n",
    "$$\n",
    "\\nabla_{\\theta}J(\\theta) =\\sum_{\\left(t_{1:T},a_T\\right)\\in\\mathbb{A}^+}\\pi\\left(t_{1:T},a_T;\\theta\\right)\\left[\\nabla_{\\theta}log\\pi\\left(t_{1:T},a_T;\\theta\\right)\\left(r_T-b\\right)\\right]\n",
    "$$\n",
    ",where $b=\\sum_{\\left(t_{1:T},a_T\\right)\\in\\mathbb{A}^+}\\pi\\left(t_{1:T},a_T;\\theta\\right)r_T$ is the average reward on the $\\left|\\mathbb{A}^+\\right|$ episodes.\n",
    "\n",
    "Since the sum of the rewards over $\\left|\\mathbb{A}^+\\right|$ episodes is zero, $\\sum_{\\left(t_{1:T},a_T\\right)\\in\\mathbb{A}^+}\\pi\\left(t_{1:T},a_T;\\theta\\right)\\left(r_T-b\\right)=0$, they call it Contractive Reward. Further more, they found using $\\left(\\frac{r_T}{b}-1\\right)$ in replace of $\\left(r_T-b\\right)$ will lead to a better convergence.\n",
    "\n",
    "In our implementation, we take the reward in the form,\n",
    "$$\n",
    "J(\\theta)=\\sum_{\\left(t_{1:T},a_T\\right)\\in\\mathbb{A}^+}\\pi\\left(t_{1:T},a_T;\\theta\\right)\\left(\\frac{r_T}{b}-1\\right) + b\n",
    "$$\n",
    "As we only compute gradient on $\\pi\\left(t_{1:T},a_T;\\theta\\right)$ and treat other components in the formula as a constant, the derivate is the same as the paper while the output is the average rewards in $\\left|\\mathbb{A}^+\\right|$ episodes.\n",
    "In CNTK, we use stop_gradient operator over the output of a function to conver it to a constant in the math formula."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def contractive_reward(labels, predictions_and_stop_probabilities):\n",
    "  \"\"\"\n",
    "  Compute the contractive reward loss in paper 'ReasoNet: \n",
    "    Learning to Stop Reading in Machine Comprehension'\n",
    "  Args:\n",
    "    labels: The lables\n",
    "    predictions_and_stop_probabilities: A list of tuples, \n",
    "    each tuple contains the prediction and stop probability of the coresponding step.\n",
    "  \"\"\"\n",
    "  base = None\n",
    "  avg_rewards = None\n",
    "  for step in range(len(predictions_and_stop_probabilities)):\n",
    "    pred = predictions_and_stop_probabilities[step][0]\n",
    "    stop = predictions_and_stop_probabilities[step][1]\n",
    "    if base is None:\n",
    "      base = ops.element_times(pred, stop)\n",
    "    else:\n",
    "      base = ops.plus(ops.element_times(pred, stop), base)\n",
    "  avg_rewards = ops.stop_gradient(sequence.reduce_sum(base*labels))\n",
    "  base_reward = sequence.broadcast_as(avg_rewards, base, name = 'base_line')\n",
    "  # While  the learner will mimize the loss by default, we want it to maxiumize the rewards\n",
    "  # Maxium rewards => minimal -rewards\n",
    "  # So we use (1-r/b) as the rewards instead of (r/b-1)\n",
    "  step_cr = ops.stop_gradient(1- ops.element_divide(labels, base_reward))\n",
    "  normalized_contractive_rewards = ops.element_times(base, step_cr)\n",
    "  rewards = sequence.reduce_sum(normalized_contractive_rewards) + avg_rewards\n",
    "  return rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def accuracy_func(prediction, label, name='accuracy'):\n",
    "  \"\"\"\n",
    "  Compute the accuracy of the prediction\n",
    "  \"\"\"\n",
    "  pred_max = ops.hardmax(prediction, name='pred_max')\n",
    "  norm_label = ops.equal(label, [1], name='norm_label')\n",
    "  acc = ops.times_transpose(pred_max, norm_label, name='accuracy')\n",
    "  return acc\n",
    "\n",
    "def loss(model, params:model_params):\n",
    "  \"\"\"\n",
    "  Compute the loss and accuracy of the model output\n",
    "  \"\"\"\n",
    "  model_args = {arg.name:arg for arg in model.arguments}\n",
    "  context = model_args['context']\n",
    "  entity_ids_mask = model_args['entity_ids_mask']\n",
    "  entity_condition = greater(entity_ids_mask, 0, name='condidion')\n",
    "  # Get all the enities in the paragraph via gather operator, which will create a new dynamic sequence axis \n",
    "  entities_all = sequence.gather(entity_condition, entity_condition, name='entities_all')\n",
    "  # The generated dynamic axis has the same length as the input enity id sequence, \n",
    "  # so we asign it as the entity id's dynamic axis.\n",
    "  entity_ids = Input(shape=(params.entity_dim), is_sparse=True, \n",
    "                              dynamic_axes=entities_all.dynamic_axes, name='entity_ids')\n",
    "  wordvocab_dim = params.vocab_dim\n",
    "  labels_raw = Input(shape=(1,), is_sparse=False, dynamic_axes=context.dynamic_axes, \n",
    "                              name='labels')\n",
    "  #answers = sequence.scatter(sequence.gather(model.outputs[-1], entity_condition), entities_all, name='Final_Ans')\n",
    "  #labels = sequence.scatter(sequence.gather(labels_raw, entity_condition), entities_all, name='EntityLabels')\n",
    "  answers = sequence.gather(model.outputs[-1], entity_condition, \n",
    "                             name='Final_Ans')\n",
    "  labels = sequence.gather(labels_raw, entity_condition, \n",
    "                            name='EntityLabels')\n",
    "  entity_id_matrix = ops.reshape(entity_ids, params.entity_dim)\n",
    "  expand_pred = sequence.reduce_sum(element_times(answers, entity_id_matrix))\n",
    "  expand_label = ops.greater_equal(sequence.reduce_sum(element_times(labels, entity_id_matrix)), 1)\n",
    "  expand_candidate_mask = ops.greater_equal(sequence.reduce_sum(entity_id_matrix), 1)\n",
    "  predictions_and_stop_probabilities=[]\n",
    "  for step in range(int((len(model.outputs)-1)/2)):\n",
    "    predictions_and_stop_probabilities += [(model.outputs[step*2], model.outputs[step*2+1])]\n",
    "  loss_value = contractive_reward(labels_raw, predictions_and_stop_probabilities)\n",
    "  accuracy = accuracy_func(expand_pred, expand_label, name='accuracy')\n",
    "  apply_loss = combine([loss_value, answers, labels, accuracy], name='Loss')\n",
    "  return apply_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adam Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_adam_learner(learn_params, learning_rate = 0.0005, gradient_clipping_threshold_per_sample=0.001):\n",
    "  \"\"\"\n",
    "  Create adam learner\n",
    "  \"\"\"\n",
    "  lr_schedule = learners.learning_rate_schedule(learning_rate, learners.UnitType.sample)\n",
    "  momentum = learners.momentum_schedule(0.90)\n",
    "  gradient_clipping_threshold_per_sample = gradient_clipping_threshold_per_sample\n",
    "  gradient_clipping_with_truncation = True\n",
    "  momentum_var = learners.momentum_schedule(0.999)\n",
    "  lr = learners.adam(learn_params, lr_schedule, momentum, True, momentum_var,\n",
    "          gradient_clipping_threshold_per_sample = gradient_clipping_threshold_per_sample,\n",
    "          gradient_clipping_with_truncation = gradient_clipping_with_truncation)\n",
    "  learner_desc = 'Alg: Adam, learning rage: {0}, momentum: {1}, gradient clip: {2}'\\\n",
    "    .format(learning_rate, momentum[0], gradient_clipping_threshold_per_sample)\n",
    "  logger.log(\"Create learner. {0}\".format(learner_desc))\n",
    "  return lr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Reader\n",
    "The data is stored in CNTK Text Format and we need to create a reader to consume the data. There are 5 columns/streams in the data file, e.g. *context*, *query*, *entity indication*, *label*, *entity ids*. And we use `bind_data` function to bind the *streams* with CNTK functions' (e.g. *model*, *loss*) *arguments* based on their names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_reader(path, vocab_dim, entity_dim, randomize, rand_size= DEFAULT_RANDOMIZATION_WINDOW, size=INFINITELY_REPEAT):\n",
    "  \"\"\"\n",
    "  Create data reader for the model\n",
    "  Args:\n",
    "    path: The data path\n",
    "    vocab_dim: The dimention of the vocabulary\n",
    "    entity_dim: The dimention of entities\n",
    "    randomize: Where to shuffle the data before feed into the trainer\n",
    "  \"\"\"\n",
    "  return MinibatchSource(CTFDeserializer(path, StreamDefs(\n",
    "    context  = StreamDef(field='C', shape=vocab_dim, is_sparse=True),\n",
    "    query    = StreamDef(field='Q', shape=vocab_dim, is_sparse=True),\n",
    "    entities  = StreamDef(field='E', shape=1, is_sparse=False),\n",
    "    label   = StreamDef(field='L', shape=1, is_sparse=False),\n",
    "    entity_ids   = StreamDef(field='EID', shape=entity_dim, is_sparse=True)\n",
    "    )), randomize=randomize)\n",
    "\n",
    "def bind_data(func, data):\n",
    "  \"\"\"\n",
    "  Bind data outputs to cntk function arguments based on the argument name\n",
    "  \"\"\"\n",
    "  bind = {}\n",
    "  for arg in func.arguments:\n",
    "    if arg.name == 'query':\n",
    "      bind[arg] = data.streams.query\n",
    "    if arg.name == 'context':\n",
    "      bind[arg] = data.streams.context\n",
    "    if arg.name == 'entity_ids_mask':\n",
    "      bind[arg] = data.streams.entities\n",
    "    if arg.name == 'labels':\n",
    "      bind[arg] = data.streams.label\n",
    "    if arg.name == 'entity_ids':\n",
    "      bind[arg] = data.streams.entity_ids\n",
    "  return bind"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def __evaluation(trainer, data, bind, minibatch_size, epoch_size):\n",
    "  \"\"\"\n",
    "  Evaluate the loss and accurate of the evaluation data set during training stage\n",
    "  \"\"\"\n",
    "  if epoch_size is None:\n",
    "    epoch_size = 1\n",
    "  for key in bind.keys():\n",
    "    if key.name == 'labels':\n",
    "      label_arg = key\n",
    "      break\n",
    "  eval_acc = 0\n",
    "  eval_s = 0\n",
    "  k = 0\n",
    "  print(\"Start evaluation with {0} samples ...\".format(epoch_size))\n",
    "  while k < epoch_size:\n",
    "    mbs = min(epoch_size - k, minibatch_size)\n",
    "    mb = data.next_minibatch(mbs, input_map=bind)\n",
    "    k += mb[label_arg].num_samples\n",
    "    sm = mb[label_arg].num_sequences\n",
    "    avg_acc = trainer.test_minibatch(mb)\n",
    "    eval_acc += sm*avg_acc\n",
    "    eval_s += sm\n",
    "    sys.stdout.write('.')\n",
    "    sys.stdout.flush()\n",
    "  eval_acc /= eval_s\n",
    "  print(\"\")\n",
    "  logger.log(\"Evaluation Acc: {0}, samples: {1}\".format(eval_acc, eval_s))\n",
    "  return eval_acc\n",
    "\n",
    "def train(model, m_params:model_params, learner, train_data, max_epochs=1, \n",
    "          save_model_flag=False, epoch_size=270000, eval_data=None, eval_size=None, \n",
    "          check_point_freq=0.1, minibatch_size=50000, model_name='rsn'):\n",
    "  \"\"\"\n",
    "  Train the model\n",
    "  Args:\n",
    "    model: The created model\n",
    "    m_params: Model parameters\n",
    "    learner: The learner used to train the model\n",
    "  \"\"\"\n",
    "  criterion_loss = loss(model, m_params)\n",
    "  loss_func = criterion_loss.outputs[0]\n",
    "  eval_func = criterion_loss.outputs[-1]\n",
    "  trainer = Trainer(model.outputs[-1], (loss_func, eval_func), learner)\n",
    "  # Get minibatches of sequences to train with and perform model training\n",
    "  # bind inputs to data from readers\n",
    "  train_bind = bind_data(criterion_loss, train_data)\n",
    "  for k in train_bind.keys():\n",
    "    if k.name == 'labels':\n",
    "      label_key = k\n",
    "      break\n",
    "  eval_bind = bind_data(criterion_loss, eval_data)\n",
    "\n",
    "  i = 0\n",
    "  minibatch_count = 0\n",
    "  training_progress_output_freq = 500\n",
    "  check_point_interval = int(epoch_size*check_point_freq)\n",
    "  check_point_id = 0\n",
    "  for epoch in range(max_epochs):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    epoch_samples = 0\n",
    "    i = 0\n",
    "    win_loss = 0\n",
    "    win_acc = 0\n",
    "    win_samples = 0\n",
    "    chk_loss = 0\n",
    "    chk_acc = 0\n",
    "    chk_samples = 0\n",
    "    while i < epoch_size:\n",
    "      # get next minibatch of training data\n",
    "      mbs = min(minibatch_size, epoch_size - i)\n",
    "      mb_train = train_data.next_minibatch(minibatch_size, input_map=train_bind)\n",
    "      i += mb_train[label_key].num_samples\n",
    "      trainer.train_minibatch(mb_train)\n",
    "      minibatch_count += 1\n",
    "      sys.stdout.write('.')\n",
    "      sys.stdout.flush()\n",
    "      # collect epoch-wide stats\n",
    "      samples = trainer.previous_minibatch_sample_count\n",
    "      ls = trainer.previous_minibatch_loss_average * samples\n",
    "      acc = trainer.previous_minibatch_evaluation_average * samples\n",
    "      epoch_loss += ls\n",
    "      epoch_acc += acc\n",
    "      win_loss += ls\n",
    "      win_acc += acc\n",
    "      chk_loss += ls\n",
    "      chk_acc += acc\n",
    "      epoch_samples += samples\n",
    "      win_samples += samples\n",
    "      chk_samples += samples\n",
    "      if int(epoch_samples/training_progress_output_freq) != \\\n",
    "        int((epoch_samples-samples)/training_progress_output_freq):\n",
    "        print('')\n",
    "        logger.log(\"Lastest sample count = {}, Train Loss: {}, Evalualtion ACC: {}\"\\\n",
    "                   .format(win_samples, win_loss/win_samples,\n",
    "          win_acc/win_samples))\n",
    "        logger.log(\"Total sample count = {}, Train Loss: {}, Evalualtion ACC: {}\"\\\n",
    "                   .format(chk_samples, chk_loss/chk_samples,\n",
    "          chk_acc/chk_samples))\n",
    "        win_samples = 0\n",
    "        win_loss = 0\n",
    "        win_acc = 0\n",
    "      new_chk_id = int(i/check_point_interval)\n",
    "      if new_chk_id != check_point_id and i < epoch_size :\n",
    "        check_point_id = new_chk_id\n",
    "        print('')\n",
    "        logger.log(\"--- CHECKPOINT %d: samples=%d, loss = %.2f, acc = %.2f%% ---\" % (check_point_id, \n",
    "                                                                                     chk_samples, \n",
    "                                                                                     chk_loss/chk_samples, \n",
    "                                                                                     100.0*(chk_acc/chk_samples)))\n",
    "        if eval_data:\n",
    "          __evaluation(trainer, eval_data, eval_bind, minibatch_size, eval_size)\n",
    "        if save_model_flag:\n",
    "          # save the model every epoch\n",
    "          model_filename = os.path.join('model', \"model_%s_%03d.dnn\" % (model_name, check_point_id))\n",
    "          model.save_model(model_filename)\n",
    "          logger.log(\"Saved model to '%s'\" % model_filename)\n",
    "        chk_samples = 0\n",
    "        chk_loss = 0\n",
    "        chk_acc = 0\n",
    "\n",
    "    print('')\n",
    "    logger.log(\"--- EPOCH %d: samples=%d, loss = %.2f, acc = %.2f%% ---\" % (epoch, epoch_samples,\n",
    "                                                                            epoch_loss/epoch_samples,\n",
    "                                                                            100.0*(epoch_acc/epoch_samples)))\n",
    "  eval_acc = 0\n",
    "  if eval_data:\n",
    "    eval_acc = __evaluation(trainer, eval_data, eval_bind, minibatch_size, eval_size)\n",
    "  if save_model_flag:\n",
    "    # save the model every epoch\n",
    "    model_filename = os.path.join('model', \"model_%s_final.dnn\" % (model_name))\n",
    "    model.save_model(model_filename)\n",
    "    logger.log(\"Saved model to '%s'\" % model_filename)\n",
    "  return (epoch_loss/epoch_samples, epoch_acc/epoch_samples, eval_acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test all the components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log with log file: log/train_03-24_09.02.30.log\n",
      "Create model: dropout_rate: 0.2, init:<cntk.cntk_py.Dictionary; proxy of <Swig Object of type 'CNTK::ImageTransform *' at 0x7f5d40654900> >, embedding_init: None\n",
      "Create learner. Alg: Adam, learning rage: 0.0005, momentum: 0.9, gradient clip: 0.001\n",
      ".........................................................................................................................................................................................................................................................\n",
      "Lastest sample count = 500, Train Loss: 0.10945142503883108, Evalualtion ACC: 0.296\n",
      "Total sample count = 500, Train Loss: 0.10945142503883108, Evalualtion ACC: 0.296\n",
      "..........................................................................................................................................................................................................................................................\n",
      "Lastest sample count = 501, Train Loss: 0.13228679520364742, Evalualtion ACC: 0.26746506986027946\n",
      "Total sample count = 1001, Train Loss: 0.12088051640004285, Evalualtion ACC: 0.2817182817182817\n",
      "........................................................................................................................................................................................................................................................\n",
      "Lastest sample count = 499, Train Loss: 0.15035069163003095, Evalualtion ACC: 0.2845691382765531\n",
      "Total sample count = 1500, Train Loss: 0.13068426135988556, Evalualtion ACC: 0.2826666666666667\n",
      ".............................................................................."
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import cntk.device as device\n",
    "import numpy as np\n",
    "from cntk.ops.tests.ops_test_utils import cntk_device\n",
    "from cntk.ops import input_variable, past_value, future_value\n",
    "from cntk.io import MinibatchSource\n",
    "from cntk import Trainer, Axis, device, combine\n",
    "from cntk.layers import Recurrence, Convolution\n",
    "import cntk.ops as ops\n",
    "import cntk\n",
    "import math\n",
    "\n",
    "def test_reasonet():  \n",
    "  data_path = train_ctf\n",
    "  eval_path = validation_ctf\n",
    "  vocab_dim = 101585\n",
    "  entity_dim = 586\n",
    "  epoch_size=289716292\n",
    "  eval_size=2993016\n",
    "  hidden_dim=384\n",
    "  max_rl_steps=5\n",
    "  max_epochs=5\n",
    "  embedding_dim=100\n",
    "  att_dim = 384\n",
    "  demo_only=True\n",
    "  if demo_only:\n",
    "    # Use a smaller minibatch_size to reduce memory usage for demo popurse only\n",
    "    minibatch_size=2000\n",
    "    hidden_dim = 256\n",
    "    att_dim = 256\n",
    "    max_rl_steps = 3\n",
    "    max_epochs = 1\n",
    "  else:\n",
    "    # The average sequence length is about 700, so we set the minibatch_size to 50000 in sequence num,\n",
    "    # which is about 70 samples/instanes per minibatch\n",
    "    minibatch_size=50000  \n",
    "    \n",
    "  params = model_params(vocab_dim = vocab_dim, entity_dim = entity_dim, hidden_dim = hidden_dim, \n",
    "                        embedding_dim = embedding_dim, att_dim=att_dim, max_rl_steps = max_rl_steps,\n",
    "                        embedding_init = None, dropout_rate = 0.2)\n",
    "\n",
    "  train_data = create_reader(data_path, vocab_dim, entity_dim, True, rand_size=epoch_size)\n",
    "  eval_data = create_reader(eval_path, vocab_dim, entity_dim, False, rand_size=eval_size) \\\n",
    "if eval_path is not None else None\n",
    "  embedding_init = None\n",
    "\n",
    "  model = create_model(params)\n",
    "  learner = create_adam_learner(model.parameters)\n",
    "  (train_loss, train_acc, eval_acc) = train(model, params, learner, train_data, \n",
    "                                            max_epochs=max_epochs, epoch_size=epoch_size, \n",
    "                                            save_model_flag=False, model_name=os.path.basename(data_path),\n",
    "                                            eval_data=eval_data, eval_size=eval_size, check_point_freq=0.1,\n",
    "                                            minibatch_size = minibatch_size)\n",
    "\n",
    "test_reasonet()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:cntk-py34]",
   "language": "python",
   "name": "conda-env-cntk-py34-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
